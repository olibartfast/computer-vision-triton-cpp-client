#!/bin/bash
# chmod +x docker_triton_run.sh
# ./docker_triton_run.sh <path_to_models_host> <triton_version> [cpu|gpu] [python_backend]
# 
# Examples:
# MODEL_REPOSITORY_HOST=$HOME/model_repository
# TRITON_VERSION=25.06
# ./docker_triton_run.sh $MODEL_REPOSITORY_HOST $TRITON_VERSION cpu
# ./docker_triton_run.sh $MODEL_REPOSITORY_HOST $TRITON_VERSION gpu
# ./docker_triton_run.sh $MODEL_REPOSITORY_HOST $TRITON_VERSION gpu python_backend

set -euo pipefail  # Exit on error, undefined vars, pipe failures

# Default values
MODEL_REPO=$1
TRITON_VERSION=${2:-25.06}
DEVICE_TYPE=${3:-cpu}
PYTHON_BACKEND_ARG=${4:-false}

# Cleanup function for graceful shutdown
cleanup() {
    echo ""
    echo "ðŸ›‘ Shutting down Triton Server..."
}
trap cleanup EXIT INT TERM

# Validate required parameters
if [ $# -lt 1 ]; then
    echo "Usage: $0 <model_repository_path> [triton_version] [cpu|gpu] [python_backend|true]"
    echo ""
    echo "Examples:"
    echo "  $0 \$HOME/model_repository 25.06 cpu"
    echo "  $0 \$HOME/model_repository 25.06 gpu python_backend"
    echo "  $0 \$HOME/model_repository 25.06 gpu true"
    echo ""
    echo "Parameters:"
    echo "  model_repository_path: Path to your model repository (required)"
    echo "  triton_version: Docker image version (default: 25.06)"
    echo "  device_type: cpu or gpu (default: cpu)"
    echo "  python_backend: python_backend|true|1|yes to enable (default: false)"
    exit 1
fi

# Validate dependencies
if ! command -v docker &> /dev/null; then
    echo "âŒ Error: Docker not found. Please install Docker first."
    exit 1
fi

# Validate model repository
if [ ! -d "$MODEL_REPO" ]; then
    echo "âŒ Error: Model repository not found: $MODEL_REPO"
    echo "Please ensure the directory exists and is accessible."
    exit 1
fi

# Normalize device type
case "${DEVICE_TYPE,,}" in  # Convert to lowercase
    gpu|cuda) DEVICE_TYPE="gpu" ;;
    cpu|x86|x64) DEVICE_TYPE="cpu" ;;
    *) 
        echo "âŒ Error: Invalid device type '$DEVICE_TYPE'. Use 'cpu' or 'gpu'."
        exit 1
        ;;
esac

# Normalize python backend parameter
case "${PYTHON_BACKEND_ARG,,}" in
    python_backend|true|1|yes|on) PYTHON_BACKEND=true ;;
    *) PYTHON_BACKEND=false ;;
esac

# Validate GPU availability if requested
if [ "$DEVICE_TYPE" = "gpu" ]; then
    echo "ðŸ” Checking GPU availability..."
    if ! docker run --rm --gpus=all --entrypoint nvidia-smi nvcr.io/nvidia/cuda:12.2-base-ubuntu20.04 &>/dev/null; then
        echo "âš ï¸  Warning: GPU not available or nvidia-container-toolkit not configured properly"
        echo "ðŸ’¡ Falling back to CPU mode. To fix:"
        echo "   1. Install nvidia-container-toolkit"
        echo "   2. Restart Docker daemon"
        echo "   3. Ensure NVIDIA drivers are installed"
        DEVICE_TYPE="cpu"
        sleep 2
    else
        echo "âœ… GPU support confirmed"
    fi
fi

# Display configuration
echo ""
echo "ðŸš€ Starting Triton Inference Server"
echo "ðŸ“ Model repository: $MODEL_REPO"
echo "ðŸ·ï¸  Triton version: $TRITON_VERSION"
echo "ðŸ’» Device: $DEVICE_TYPE"
echo "ðŸ Python backend: $PYTHON_BACKEND"
echo ""

# Determine Docker image and setup
BASE_IMAGE="nvcr.io/nvidia/tritonserver:$TRITON_VERSION-py3"

if [ "$PYTHON_BACKEND" = "true" ]; then
    echo "ðŸ“¦ Python backend enabled - packages will be installed at startup"
    echo "ðŸ’¡ Consider creating a custom image for production use"
    PYTHON_INSTALL_CMD="pip install --no-cache-dir torch torchvision transformers pillow numpy scipy && echo 'âœ… Python packages installed' && "
else
    echo "âš¡ Skipping Python backend package installation"
    PYTHON_INSTALL_CMD=""
fi

# Build Docker command components
DOCKER_BASE="docker run --rm --name triton-server-$$"
DOCKER_PORTS="-p8000:8000 -p8001:8001 -p8002:8002"
DOCKER_VOLUME="-v \"$MODEL_REPO\":/models"
DOCKER_IMAGE="$BASE_IMAGE"

# Add GPU support if needed
if [ "$DEVICE_TYPE" = "gpu" ]; then
    echo "ðŸŽ® Using GPU acceleration"
    DOCKER_GPU="--gpus=all"
else
    echo "ðŸ–¥ï¸  Using CPU only"
    DOCKER_GPU=""
fi

# Triton server arguments
TRITON_ARGS="--model-repository=/models"

# Add CPU-specific optimizations
if [ "$DEVICE_TYPE" = "cpu" ]; then
    TRITON_ARGS="$TRITON_ARGS --backend-config=tensorflow,allow_soft_placement=true"
fi

# Build final command
if [ -n "$PYTHON_INSTALL_CMD" ]; then
    FULL_CMD="$DOCKER_BASE $DOCKER_GPU $DOCKER_PORTS $DOCKER_VOLUME $DOCKER_IMAGE bash -c \"$PYTHON_INSTALL_CMD tritonserver $TRITON_ARGS\""
else
    FULL_CMD="$DOCKER_BASE $DOCKER_GPU $DOCKER_PORTS $DOCKER_VOLUME $DOCKER_IMAGE tritonserver $TRITON_ARGS"
fi

# Display the command being executed (for debugging)
echo "ðŸ”§ Docker command:"
echo "$FULL_CMD"
echo ""

# Pull the latest image if it doesn't exist locally
echo "ðŸ”„ Ensuring Docker image is available..."
if ! docker image inspect "$BASE_IMAGE" &>/dev/null; then
    echo "ðŸ“¥ Pulling $BASE_IMAGE..."
    docker pull "$BASE_IMAGE"
fi

echo "ðŸŒŸ Starting Triton Server..."
echo "ðŸ“¡ Server will be available at:"
echo "   HTTP:  http://localhost:8000"
echo "   GRPC:  localhost:8001"  
echo "   Metrics: http://localhost:8002/metrics"
echo ""
echo "ðŸ›‘ Press Ctrl+C to stop the server"
echo ""

# Execute the command
eval "$FULL_CMD"